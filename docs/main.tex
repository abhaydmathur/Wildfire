\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{booktabs, multirow} 
\usepackage{amsmath,amssymb}
\DeclareMathOperator{\E}{\mathbb{E}}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


%% Title
\title{We Didn't Start The Fire
%%%% Cite as
% %%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Ivanina Ivanova, Abhay Mathur \\
  Institut Polytechnique de Paris \\
  \texttt{\{abhay.mathur, ivanina.ivanova\}@ip-paris.fr} \\
  %% examples of more authors
}


\begin{document}
\maketitle

\begin{abstract}
  \lipsum[1]
\end{abstract}

% keywords can be removed
\keywords{First keyword \and Second keyword \and More}

\section{Introduction: Problem Statement}
This project aims to develop a fire detection system using a designated
dataset. The dataset consists of three subsets: a training set, a validation
set, and a test set. Specific constraints and guidelines must be strictly
followed to ensure compliance with the project requirements.

\subsection{Dataset Access and Constraints}

The dataset is available for download at
\url{https://www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset/code}.
It comprises a training set, a validation set, and a test set. A critical
restriction is imposed on the training set: its labels are inaccessible. Any
direct utilization of annotations from the training set will lead to
disqualification.

\subsection{Dataset Partitioning}

To facilitate model training, the original validation set must be partitioned
into a newly defined validation set and a new training set. The original
training set may be leveraged in a creative manner; however, its labels must
not be employed under any circumstances.

\subsection{Model Development}

A deep neural network (DNN) will be trained utilizing the newly defined
training and validation sets. Various methodologies and supplementary resources
may be incorporated to enhance model performance, provided that all constraints
related to annotation usage are rigorously upheld.

\section{Method}\label{sec:method}

\subsection{Dataset Analysis}
\dots

\subsection{Proposition 1: Using Available Labeled Data}
\subsubsection{Naive Coordinates Classifier}
\dots

\subsubsection{ResNet Classifier}
\dots

\subsection{Self-Supervision (Learning from Unlabeled Data)}

\subsection{SimCLR}
Just wanted to cite this paper~\cite{simclr}.

\subsection{Variational Autoencoder (VAE) + Classifier}

A Variational Autoencoder (VAE) was employed as a self-supervised learning approach to extract latent representations from the training dataset. The VAE comprises an encoder network, which maps input images to a latent space distribution, and a decoder network, which reconstructs the input from latent variables. The encoder network consists of a series of 4 convolutional layers followed by fully connected layers, while the decoder network mirrors the encoder architecture. The VAE was trained using the unlabeled training dataset. The training objective that we decided to use is Beta-VAE~\cite{beta-vae}. Beta-VAE is a modification of the original VAE that introduces a hyperparameter $\beta$ to the loss function. The $\beta$ parameter controls the trade-off between the reconstruction loss and the Kullback-Leibler divergence. The loss function for the Beta-VAE is given by:
$$\mathcal{L}(\theta, \phi, x, z, \beta) = \E_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta D_{KL}(q_\phi(z|x)\| p(z))$$

where $p(z|x)$ is the learned posterior, $p(z)$ is the prior distribution (assumed to be Gaussian), and $p(x|z$) represents the likelihood of reconstructing the input image from the latent space. Well chosen $\beta$ values can lead to disentangled representations in the latent space. The VAE was trained for 50 epochs using the Adam optimizer with a learning rate of 0.0001. The latent representations obtained from the VAE were then used to train a classifier. The classifier consists of a two fully connected layer with a ReLU activation fucntion and a Dropout layer in between. The classifier was trained using the labeled train dataset. The training objective was to minimize the binary cross-entropy loss between the predicted and true labels. The linear classifier was trained for 20 epochs using the Adam optimizer with a learning rate of 0.0001.


\begin{table}[!htp]\centering
  \caption{Comparisons}\label{tab: }
  \scriptsize
  \begin{tabular}{lrrrr}\toprule
                                               & \textbf{Model}  & \textbf{Accuracy} & \textbf{F1 Score} \\\cmidrule{2-4}
    \multirow{2}{*}{\textbf{No pre-training}}  & Coords Only     & 0.855             & 0.873             \\ %\cmidrule{2-4}
                                               & ResNet50        & 0.985             & 0.986             \\\cmidrule{1-4}
    \multirow{3}{*}{\textbf{Self-Supervision}} & SimCLR+ResNet50 & ---               & ---               \\ %\cmidrule{2-4}
                                               & VAE             & ---               & ---               \\ %\cmidrule{2-4}
                                               & Something Else  & ---               & ---               \\\midrule
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
Your conclusion here

%Bibliography
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
