{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta file: /data/amathur-23/ROB313/train_unlabeled.csv\n",
      "Loading meta file: /data/amathur-23/ROB313/train.csv\n",
      "Loading meta file: /data/amathur-23/ROB313/val.csv\n",
      "Loading meta file: /data/amathur-23/ROB313/test.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.datasets import WildfireDataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = WildfireDataset('/data/amathur-23/ROB313', split='train', labeled=False, transforms=transform)\n",
    "data_train_labeled = WildfireDataset('/data/amathur-23/ROB313', split='train', labeled=True, transforms=transform)\n",
    "val_dataset = WildfireDataset('/data/amathur-23/ROB313', split='val', transforms=transform)\n",
    "test_dataset = WildfireDataset('/data/amathur-23/ROB313', split='test', transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader_labeled = DataLoader(data_train_labeled, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # 3x224x224\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1), # 224 -> 112\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1), # 112 -> 56\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # 56 -> 28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 28 -> 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 4, stride=2, padding=1),  # 14 -> 7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder_output_dim = (256 * 7 * 7)\n",
    "        self.fc_mu = nn.Linear(self.encoder_output_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.encoder_output_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.Linear(256, self.encoder_output_dim)\n",
    "        ) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(batch_size,-1)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(x.size(0), 256, 7, 7) \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAELoss(nn.Module):\n",
    "    def __init__(self, beta=1):\n",
    "        super(BetaVAELoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x, recon_x, mu, logvar):\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + self.beta * kl_loss\n",
    "    \n",
    "criterion_vae = BetaVAELoss(beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, f\"Training {epoch}\"):\n",
    "        data = batch['image'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = criterion_vae(data, recon_batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, f\"Validation {epoch}\"):\n",
    "            data = batch['image'].to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = criterion_vae(data, recon_batch, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 0:  60%|██████    | 285/473 [01:27<00:55,  3.36it/s]"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, device, epoch)\n",
    "    print(f\"Epoch {epoch} Train loss: {train_loss}\")\n",
    "    val_loss = validate(model, val_loader, device, epoch)\n",
    "    print(f\"Epoch {epoch} Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "def perform_clustering(features, method=\"kmeans\", num_clusters=5):\n",
    "    if method == \"kmeans\":\n",
    "        clustering = KMeans(n_clusters=num_clusters, random_state=42).fit(features)\n",
    "    elif method == \"gmm\":\n",
    "        clustering = GaussianMixture(n_components=num_clusters, random_state=42).fit(features)\n",
    "    elif method == \"dbscan\":\n",
    "        clustering = DBSCAN(eps=0.5, min_samples=5).fit(features)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported clustering method\")\n",
    "    return clustering.labels_\n",
    "\n",
    "\n",
    "labels = perform_clustering(labelled_features, method=\"kmeans\", num_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vae, device, input_dim=128, dropout=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.vae = vae.to(device)  \n",
    "        self.vae.eval()  \n",
    "        self.device = device\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x['image'].to(self.device)  \n",
    "            mu, _ = self.vae.encode(x)  \n",
    "        return self.fc(mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def train_classifier(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        target = batch['label'].float().to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch).squeeze()  \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = (output > 0.5).float() \n",
    "        correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "def validate_classifier(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            target = batch['label'].float().to(device)\n",
    "            output = model(batch).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()  \n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "            \n",
    "        f1 = f1_score(np.concatenate(all_targets), np.concatenate(all_preds))\n",
    "        print(f'Validation Loss: {total_loss / len(val_loader)}')\n",
    "        print(f'Validation Accuracy: {100. * correct / total}')\n",
    "        print(f'Validation F1 Score: {f1}')\n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train loss: 0.5874448045145108, Train accuracy: 79.32539682539682\n",
      "Epoch 1 Train loss: 0.40600709824622433, Train accuracy: 84.90079365079364\n",
      "Epoch 2 Train loss: 0.36300089634671995, Train accuracy: 85.51587301587301\n",
      "Epoch 3 Train loss: 0.35249387916130354, Train accuracy: 86.0515873015873\n",
      "Epoch 4 Train loss: 0.3499677730134771, Train accuracy: 86.0515873015873\n",
      "Epoch 5 Train loss: 0.34155348943004127, Train accuracy: 86.64682539682539\n",
      "Epoch 6 Train loss: 0.33647876959058304, Train accuracy: 86.74603174603175\n",
      "Epoch 7 Train loss: 0.3341712498966652, Train accuracy: 87.14285714285714\n",
      "Epoch 8 Train loss: 0.32906397573555574, Train accuracy: 87.00396825396825\n",
      "Epoch 9 Train loss: 0.32455714971204347, Train accuracy: 87.42063492063492\n",
      "Epoch 10 Train loss: 0.3188320213480841, Train accuracy: 87.93650793650794\n",
      "Epoch 11 Train loss: 0.3127593507495108, Train accuracy: 88.11507936507937\n",
      "Epoch 12 Train loss: 0.30810986063148404, Train accuracy: 88.25396825396825\n",
      "Epoch 13 Train loss: 0.30499562092974214, Train accuracy: 88.17460317460318\n",
      "Epoch 14 Train loss: 0.2987814277787752, Train accuracy: 88.5515873015873\n",
      "Epoch 15 Train loss: 0.294257006690472, Train accuracy: 88.9484126984127\n",
      "Epoch 16 Train loss: 0.2901286748768408, Train accuracy: 89.04761904761905\n",
      "Epoch 17 Train loss: 0.28596335083623475, Train accuracy: 89.24603174603175\n",
      "Epoch 18 Train loss: 0.28045257831676096, Train accuracy: 89.44444444444444\n",
      "Epoch 19 Train loss: 0.27277929939423934, Train accuracy: 90.17857142857143\n",
      "Epoch 20 Train loss: 0.2729332479117792, Train accuracy: 90.13888888888889\n",
      "Epoch 21 Train loss: 0.26740692933148974, Train accuracy: 90.17857142857143\n",
      "Epoch 22 Train loss: 0.2621689160602002, Train accuracy: 90.7936507936508\n",
      "Epoch 23 Train loss: 0.2558370836173432, Train accuracy: 90.89285714285714\n",
      "Epoch 24 Train loss: 0.25590249198147014, Train accuracy: 90.91269841269842\n",
      "Epoch 25 Train loss: 0.252236005437525, Train accuracy: 90.97222222222223\n",
      "Epoch 26 Train loss: 0.24527235065079941, Train accuracy: 91.25\n",
      "Epoch 27 Train loss: 0.24296942937977706, Train accuracy: 91.28968253968254\n",
      "Epoch 28 Train loss: 0.232565837193139, Train accuracy: 92.06349206349206\n",
      "Epoch 29 Train loss: 0.234072457668902, Train accuracy: 91.92460317460318\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(model, device, input_dim=256)\n",
    "optimizer_classifier = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "criterion_classifier = nn.BCELoss()\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_classifier(classifier, train_loader_labeled, optimizer_classifier, criterion_classifier, device)\n",
    "    print(f\"Epoch {epoch} Train loss: {train_loss}, Train accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3044843375682831\n",
      "Validation Accuracy: 90.31746031746032\n",
      "Validation F1 Score: 0.9122302158273381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3044843375682831"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_classifier(classifier, val_loader, nn.BCELoss(), device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rob313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
